{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Embeddings: Raw vs. Summarized Conversation Content\n",
    "\n",
    "**Purpose:**\n",
    "1. Load a specific conversation from the 'parsed_conversations' data.\n",
    "2. Reconstruct the raw text of the conversation.\n",
    "3. Generate a summary of the conversation using GPT-4o (mimicking the pipeline).\n",
    "4. Generate embeddings for BOTH the raw text and the summary text using the\n",
    "   Google's Generative AI embeddings (mimicking the extension API).\n",
    "5. Compare the two embeddings using cosine similarity.\n",
    "\n",
    "**Troubleshooting:**\n",
    "- If you encounter errors with the Google Generative AI embeddings:\n",
    "  - Make sure you have the Google API key set in your environment as `GOOGLE_API_KEY`\n",
    "  - Or ensure you're authenticated with `gcloud auth application-default login` in your terminal\n",
    "- If you encounter errors with Azure OpenAI, ensure your API keys are properly set in environment variables\n",
    "- Update the user and conversation IDs to use ones available in your data directory\n",
    "\n",
    "**Dependencies:**\n",
    "```\n",
    "pip install google-generative-ai polars numpy python-dotenv openai scikit-learn json-repair\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import libraries and configure API clients.\n",
    "**Important:** Ensure your API keys and endpoints are correctly set up in your environment\n",
    "(e.g., using a `.env` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import textwrap\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncAzureOpenAI\n",
    "from google import genai\n",
    "from google.genai.types import EmbedContentConfig\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from json_repair import repair_json # For parsing potentially malformed JSON from LLM\n",
    "\n",
    "# Load environment variables (ensure you have a .env file or set these system-wide)\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Data Paths\n",
    "# Adjust this path to point to your data directory structure\n",
    "DATA_DIR = Path(\"../data\") # Assumes notebook is in 'notebooks' folder sibling to 'data'\n",
    "PARSED_CONV_DIR = DATA_DIR / \"dagster/parsed_conversations\"\n",
    "\n",
    "# Conversation Selection (Choose a user and conversation to analyze)\n",
    "# Replace with actual IDs from your data\n",
    "USER_ID_TO_ANALYZE = \"cm8d5ubo1000219t5pby4mssx\" # Example user ID\n",
    "CONVERSATION_ID_TO_ANALYZE = \"67b4b8b0-c57c-8010-853e-25e324c584b7\" # Example conversation ID\n",
    "\n",
    "# GPT-4o (Summarization) Configuration - Using Azure OpenAI\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = \"https://enclaveidai2163546968.openai.azure.com\"\n",
    "AZURE_OPENAI_API_VERSION = \"2024-08-01-preview\" # Use the version your deployment supports\n",
    "GPT4O_MINI_DEPLOYMENT_NAME = \"gpt-4o-mini\" # Your Azure deployment name for GPT-4o Mini\n",
    "\n",
    "# Gemini (Embedding) Configuration - Using Google Generative AI (genai) library\n",
    "# Ensure you're authenticated with gcloud auth application-default login\n",
    "GEMINI_EMBEDDING_MODEL_ID = \"text-embedding-large-exp-03-07\" # Using the latest embedding model\n",
    "# Embedding dimension\n",
    "EMBEDDING_DIMENSION = 3072  # Dimension for text-embedding-005 model\n",
    "\n",
    "# --- Client Initialization ---\n",
    "\n",
    "# Initialize Azure OpenAI Client (if keys are present)\n",
    "summarization_client = None\n",
    "if AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT:\n",
    "    summarization_client = AsyncAzureOpenAI(\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    "    print(\"Azure OpenAI client initialized for summarization.\")\n",
    "else:\n",
    "    print(\"Warning: Azure OpenAI credentials not found. Summarization will be skipped.\")\n",
    "\n",
    "embedding_client = genai.Client(vertexai=True, project=\"enclaveid\", location=\"us-central1\")\n",
    "print(\"Google Generative AI client initialized for embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function copied from the pipeline for consistency\n",
    "def get_conversation_summary_prompt_sequence(conversation: str) -> list[dict[str, str]]:\n",
    "    \"\"\"Generates the prompt for conversation summarization.\"\"\"\n",
    "    prompt_text = textwrap.dedent(\n",
    "        f\"\"\"\n",
    "          You will be given a conversation between a user and an AI assistant.\n",
    "          Your job is to provide a summary as follows:\n",
    "          1. Provide a summary that describes the progression of the conversation and what the user obtains at the end.\n",
    "          2. Determine if the conversation is highly sensitive, containing topics such as physical and mental health problems, relationship advice, erotic content, private legal matters, etc.\n",
    "          3. If the conversation is not in English, your summary should be in English.\n",
    "          4. Keep it under 150 words.\n",
    "          5. In the summary, every occurrence of \"the user\" should be replaced with \"<USER>\"\n",
    "          Use this output schema:\n",
    "          {{\n",
    "              \"summary\": str,\n",
    "              \"is_sensitive\": bool,\n",
    "          }}\n",
    "          Here is the conversation:\n",
    "          {conversation}\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "    # Using the standard message format expected by OpenAI/Azure API\n",
    "    return [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "\n",
    "# Helper function copied from the pipeline for consistency\n",
    "def parse_conversation_summaries(completion: str) -> dict | None:\n",
    "    \"\"\"Parses the LLM summary response.\"\"\"\n",
    "    try:\n",
    "        # Use json_repair for robustness against minor JSON formatting issues\n",
    "        res = repair_json(completion, return_objects=True)\n",
    "        if (\n",
    "            isinstance(res, dict)\n",
    "            and \"is_sensitive\" in res\n",
    "            and \"summary\" in res\n",
    "            and isinstance(res[\"is_sensitive\"], bool)\n",
    "            and isinstance(res[\"summary\"], str)\n",
    "        ):\n",
    "            return res\n",
    "        else:\n",
    "            print(f\"Warning: Could not parse summary response correctly. Raw: {completion}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing summary JSON: {e}. Raw: {completion}\")\n",
    "        return None\n",
    "\n",
    "# Helper function to generate embeddings using Google Generative AI\n",
    "async def generate_vertex_embedding(text: str) -> list[float] | None:\n",
    "    \"\"\"Generates embedding for a given text using Google Generative AI.\"\"\"\n",
    "    try:\n",
    "        # Use the genai library to get embeddings\n",
    "        response = embedding_client.models.embed_content(\n",
    "            model=GEMINI_EMBEDDING_MODEL_ID,\n",
    "            contents=[text],\n",
    "            config=EmbedContentConfig(\n",
    "                task_type=\"RETRIEVAL_DOCUMENT\",  # Setting the task type\n",
    "                output_dimensionality=EMBEDDING_DIMENSION,  # Setting the output dimension\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # Extract the embedding values from the response\n",
    "        if response and response.embeddings and len(response.embeddings) > 0:\n",
    "            embedding_values = response.embeddings[0].values\n",
    "            \n",
    "            if len(embedding_values) != EMBEDDING_DIMENSION:\n",
    "                print(f\"Warning: Embedding dimension mismatch. Expected {EMBEDDING_DIMENSION}, got {len(embedding_values)}\")\n",
    "                return None\n",
    "                \n",
    "            return embedding_values\n",
    "        else:\n",
    "            print(\"Warning: No embedding values returned\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating Google Generative AI embedding: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function copied from the pipeline for consistency\n",
    "def get_conversation_summary_prompt_sequence(conversation: str) -> list[dict[str, str]]:\n",
    "    \"\"\"Generates the prompt for conversation summarization.\"\"\"\n",
    "    prompt_text = textwrap.dedent(\n",
    "        f\"\"\"\n",
    "          You will be given a conversation between a user and an AI assistant.\n",
    "          Your job is to provide a summary as follows:\n",
    "          1. Provide a summary that describes the progression of the conversation and what the user obtains at the end.\n",
    "          2. Determine if the conversation is highly sensitive, containing topics such as physical and mental health problems, relationship advice, erotic content, private legal matters, etc.\n",
    "          3. If the conversation is not in English, your summary should be in English.\n",
    "          4. Keep it under 150 words.\n",
    "          5. In the summary, every occurrence of \"the user\" should be replaced with \"<USER>\"\n",
    "          Use this output schema:\n",
    "          {{\n",
    "              \"summary\": str,\n",
    "              \"is_sensitive\": bool,\n",
    "          }}\n",
    "          Here is the conversation:\n",
    "          {conversation}\n",
    "        \"\"\"\n",
    "    ).strip()\n",
    "    # Using the standard message format expected by OpenAI/Azure API\n",
    "    return [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "\n",
    "# Helper function copied from the pipeline for consistency\n",
    "def parse_conversation_summaries(completion: str) -> dict | None:\n",
    "    \"\"\"Parses the LLM summary response.\"\"\"\n",
    "    try:\n",
    "        # Use json_repair for robustness against minor JSON formatting issues\n",
    "        res = repair_json(completion, return_objects=True)\n",
    "        if (\n",
    "            isinstance(res, dict)\n",
    "            and \"is_sensitive\" in res\n",
    "            and \"summary\" in res\n",
    "            and isinstance(res[\"is_sensitive\"], bool)\n",
    "            and isinstance(res[\"summary\"], str)\n",
    "        ):\n",
    "            return res\n",
    "        else:\n",
    "            print(f\"Warning: Could not parse summary response correctly. Raw: {completion}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing summary JSON: {e}. Raw: {completion}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Conversation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the path to the user's Parquet file\n",
    "user_file_path = PARSED_CONV_DIR / f\"{USER_ID_TO_ANALYZE}.snappy\"\n",
    "\n",
    "if not user_file_path.exists():\n",
    "    raise FileNotFoundError(f\"Could not find parsed conversations for user {USER_ID_TO_ANALYZE} at {user_file_path}\")\n",
    "\n",
    "# Load the data\n",
    "df_all_convos = pl.read_parquet(user_file_path)\n",
    "print(f\"Loaded data for user {USER_ID_TO_ANALYZE}. Shape: {df_all_convos.shape}\")\n",
    "\n",
    "# Filter for the specific conversation\n",
    "df_single_convo = df_all_convos.filter(pl.col(\"conversation_id\") == CONVERSATION_ID_TO_ANALYZE).sort(\"date\", \"time\")\n",
    "\n",
    "if df_single_convo.height == 0:\n",
    "    raise ValueError(f\"Conversation ID {CONVERSATION_ID_TO_ANALYZE} not found for user {USER_ID_TO_ANALYZE}\")\n",
    "\n",
    "print(f\"Found {df_single_convo.height} messages for conversation {CONVERSATION_ID_TO_ANALYZE}\")\n",
    "\n",
    "# Reconstruct the raw conversation text\n",
    "# Format: \"QUESTION: <question text>\\nANSWER: <answer text>\\n\\nQUESTION: ...\"\n",
    "conversation_parts = []\n",
    "for row in df_single_convo.iter_rows(named=True):\n",
    "    conversation_parts.append(f\"QUESTION: {row['question']}\")\n",
    "    conversation_parts.append(f\"ANSWER: {row['answer']}\")\n",
    "\n",
    "raw_conversation_text = \"\\n\\n\".join(conversation_parts)\n",
    "\n",
    "print(\"\\n--- Raw Conversation Text (First 500 chars) ---\")\n",
    "print(textwrap.shorten(raw_conversation_text, width=500, placeholder=\"...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Conversation Summary\n",
    "\n",
    "Uses GPT-4o via Azure OpenAI to create a summary based on the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_embedding = None\n",
    "summary_text_embedding = None\n",
    "\n",
    "async def generate_embeddings():\n",
    "    global raw_text_embedding, summary_text_embedding\n",
    "    \n",
    "    print(f\"\\nGenerating embeddings using Google Generative AI model: {GEMINI_EMBEDDING_MODEL_ID}...\")\n",
    "\n",
    "    # Generate embedding for raw text\n",
    "    print(\"Embedding raw text...\")\n",
    "    raw_text_embedding = await generate_vertex_embedding(raw_conversation_text)\n",
    "    if raw_text_embedding:\n",
    "        print(f\"Raw text embedding generated. Dimension: {len(raw_text_embedding)}\")\n",
    "    else:\n",
    "        print(\"Failed to generate raw text embedding.\")\n",
    "\n",
    "    # Generate embedding for summary text (if summary exists)\n",
    "    if summary_text:\n",
    "        print(\"Embedding summary text...\")\n",
    "        summary_text_embedding = await generate_vertex_embedding(summary_text)\n",
    "        if summary_text_embedding:\n",
    "            print(f\"Summary text embedding generated. Dimension: {len(summary_text_embedding)}\")\n",
    "        else:\n",
    "            print(\"Failed to generate summary text embedding.\")\n",
    "    else:\n",
    "        print(\"Skipping summary embedding as summary was not generated.\")\n",
    "\n",
    "# Run the async function\n",
    "await generate_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Embeddings\n",
    "\n",
    "Uses the specified Gemini model via Vertex AI to generate embeddings for both the raw text and the summary text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_embedding = None\n",
    "summary_text_embedding = None\n",
    "\n",
    "async def generate_embeddings():\n",
    "    global raw_text_embedding, summary_text_embedding\n",
    "    if not embedding_client:\n",
    "        print(\"Skipping embedding generation as client is not initialized.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nGenerating embeddings using Vertex AI model: {GEMINI_EMBEDDING_MODEL_ID}...\")\n",
    "\n",
    "    # Generate embedding for raw text\n",
    "    print(\"Embedding raw text...\")\n",
    "    raw_text_embedding = await generate_vertex_embedding(raw_conversation_text)\n",
    "    if raw_text_embedding:\n",
    "        print(f\"Raw text embedding generated. Dimension: {len(raw_text_embedding)}\")\n",
    "    else:\n",
    "        print(\"Failed to generate raw text embedding.\")\n",
    "\n",
    "    # Generate embedding for summary text (if summary exists)\n",
    "    if summary_text:\n",
    "        print(\"Embedding summary text...\")\n",
    "        summary_text_embedding = await generate_vertex_embedding(summary_text)\n",
    "        if summary_text_embedding:\n",
    "            print(f\"Summary text embedding generated. Dimension: {len(summary_text_embedding)}\")\n",
    "        else:\n",
    "            print(\"Failed to generate summary text embedding.\")\n",
    "    else:\n",
    "        print(\"Skipping summary embedding as summary was not generated.\")\n",
    "\n",
    "# Run the async function\n",
    "await generate_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Embeddings\n",
    "\n",
    "Calculate the cosine similarity between the raw text embedding and the summary text embedding. A value closer to 1 indicates higher similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = None\n",
    "\n",
    "if raw_text_embedding and summary_text_embedding:\n",
    "    print(\"\\nCalculating cosine similarity...\")\n",
    "    # Reshape embeddings into 2D arrays for scikit-learn\n",
    "    embedding1 = np.array(raw_text_embedding).reshape(1, -1)\n",
    "    embedding2 = np.array(summary_text_embedding).reshape(1, -1)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = cosine_similarity(embedding1, embedding2)\n",
    "    similarity_score = similarity_matrix[0][0] # Extract the scalar value\n",
    "\n",
    "    print(f\"\\n--- Comparison Results ---\")\n",
    "    print(f\"Conversation ID: {CONVERSATION_ID_TO_ANALYZE}\")\n",
    "    print(f\"Embedding Dimension: {embedding1.shape[1]}\")\n",
    "    print(f\"Cosine Similarity (Raw vs. Summary): {similarity_score:.4f}\")\n",
    "\n",
    "    # Interpretation guide\n",
    "    if similarity_score > 0.85:\n",
    "        print(\"Interpretation: Very High Similarity - The summary captures the core semantic meaning of the raw text very well.\")\n",
    "    elif similarity_score > 0.7:\n",
    "        print(\"Interpretation: High Similarity - The summary largely reflects the meaning of the raw text.\")\n",
    "    elif similarity_score > 0.5:\n",
    "        print(\"Interpretation: Moderate Similarity - The summary captures some aspects, but there are divergences.\")\n",
    "    else:\n",
    "        print(\"Interpretation: Low Similarity - The summary and raw text have significantly different semantic representations.\")\n",
    "\n",
    "elif not raw_text_embedding:\n",
    "    print(\"\\nCannot compare embeddings: Raw text embedding was not generated.\")\n",
    "elif not summary_text_embedding:\n",
    "    print(\"\\nCannot compare embeddings: Summary text embedding was not generated.\")\n",
    "else:\n",
    "    print(\"\\nEmbeddings were not generated. Cannot perform comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook loaded a conversation, summarized it using GPT-4o, generated embeddings for both the raw and summarized versions using Gemini on Vertex AI, and calculated the cosine similarity between them.\n",
    "\n",
    "The similarity score indicates how well the summary's embedding captures the semantic essence of the raw conversation's embedding according to the chosen embedding model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}